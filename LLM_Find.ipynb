{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77738df5-e62c-4bf9-ba20-baffea46e951",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf676855-2221-47c7-8dff-b185e03c3953",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05156c09-3ec8-47d7-a629-f9374440658a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # You may need these\n",
    "# ! pip install openai\n",
    "# ! pip install langchain\n",
    "# ! pip install langchain-openai\n",
    "# ! pip install langchain-core\n",
    "# ! pip install osmnx\n",
    "# ! pip install langchain openai --upgrade\n",
    "# ! pip install itables\n",
    "# ! pip install leafmap\n",
    "# ! jupyter labextension install @jupyter-widgets/jupyterlab-manager jupyter-leaflet\n",
    "# ! pip install localtileserver \n",
    "\n",
    "## May not need these\n",
    "## ! pip install pyvis\n",
    "## ! pip install networkx\n",
    "## ! pip install OSMPythonTools\n",
    "## ! pip install contextily\n",
    "## ! pip install matplotlib_scalebar\n",
    "## ! pip install geojson\n",
    "# ! pip install toml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254e9bd-675a-4644-abfd-d642183da809",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "392efdfe-e33d-43fb-b8ea-6b6fb444ad9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhandbook\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\llm_geo\\Lib\\site-packages\\pandas\\__init__.py:37\u001b[0m\n\u001b[0;32m     30\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not built. If you want to import \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas from the source directory, you may need to run \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython setup.py build_ext\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to build the C extensions first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_err\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     38\u001b[0m     get_option,\n\u001b[0;32m     39\u001b[0m     set_option,\n\u001b[0;32m     40\u001b[0m     reset_option,\n\u001b[0;32m     41\u001b[0m     describe_option,\n\u001b[0;32m     42\u001b[0m     option_context,\n\u001b[0;32m     43\u001b[0m     options,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\llm_geo\\Lib\\site-packages\\pandas\\_config\\__init__.py:20\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mpandas._config is considered explicitly upstream of everything else in pandas,\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mshould have no intra-pandas dependencies.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mare initialized.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetect_console_encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn_copy_on_write\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m ]\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dates  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     _global_config,\n\u001b[0;32m     24\u001b[0m     describe_option,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     set_option,\n\u001b[0;32m     30\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\llm_geo\\Lib\\site-packages\\pandas\\_config\\config.py:68\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     59\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     60\u001b[0m     Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     cast,\n\u001b[0;32m     65\u001b[0m )\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     69\u001b[0m     F,\n\u001b[0;32m     70\u001b[0m     T,\n\u001b[0;32m     71\u001b[0m )\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_stack_level\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32m~\\.conda\\envs\\llm_geo\\Lib\\site-packages\\pandas\\_typing.py:198\u001b[0m\n\u001b[0;32m    192\u001b[0m Frequency \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseOffset\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    193\u001b[0m Axes \u001b[38;5;241m=\u001b[39m ListLike\n\u001b[0;32m    195\u001b[0m RandomState \u001b[38;5;241m=\u001b[39m Union[\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    197\u001b[0m     np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m--> 198\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241m.\u001b[39mGenerator,\n\u001b[0;32m    199\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mBitGenerator,\n\u001b[0;32m    200\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mRandomState,\n\u001b[0;32m    201\u001b[0m ]\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# dtypes\u001b[39;00m\n\u001b[0;32m    204\u001b[0m NpDtype \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mdtype, type_t[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mobject\u001b[39m]]]\n",
      "File \u001b[1;32m~\\.conda\\envs\\llm_geo\\Lib\\site-packages\\numpy\\__init__.py:340\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m():\n\u001b[0;32m    337\u001b[0m     public_symbols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m    338\u001b[0m     public_symbols \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatrixlib\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 340\u001b[0m         \u001b[38;5;66;03m# These were moved in 1.25 and may be deprecated eventually:\u001b[39;00m\n\u001b[0;32m    341\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleDeprecationWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisibleDeprecationWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplexWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTooHardError\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAxisError\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    343\u001b[0m     }\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(public_symbols)\n",
      "File \u001b[1;32m~\\.conda\\envs\\llm_geo\\Lib\\site-packages\\numpy\\random\\__init__.py:180\u001b[0m\n\u001b[0;32m    126\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinomial\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzipf\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    177\u001b[0m ]\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# add these for module-freeze analysis (like PyInstaller)\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pickle\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _common\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _bounded_integers\n",
      "File \u001b[1;32m~\\.conda\\envs\\llm_geo\\Lib\\site-packages\\numpy\\random\\_pickle.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmtrand\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomState\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_philox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Philox\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pcg64\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCG64, PCG64DXSM\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:1\u001b[0m, in \u001b[0;36minit numpy.random.mtrand\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import os\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import handbook\n",
    "# from pyvis.network import Network\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, HTML, Code\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import base64\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import osmnx as ox\n",
    "\n",
    "import LLM_Find_Constants as constants\n",
    "import helper\n",
    "\n",
    "import numpy as np\n",
    "# from LLM_Find_kernel import Solution\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "from time import sleep\n",
    "\n",
    "OpenAI_key = helper.load_OpenAI_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441d379-8210-4096-82af-10066688ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46f4fd-0bf8-46ca-a5f6-d4af21d28f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handbook_files = handbook.collect_handbook_files()\n",
    "# descriptions_str, data_source_dict = handbook.assemble_handbook_description(handbook_files)\n",
    "# print(descriptions_str)\n",
    "# print()\n",
    "# print(data_source_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed9a87-82d0-451b-8ce4-a703f4392520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbered_handbook_str = handbook.collect_a_handbook(source_ID='OpenStreetMap')\n",
    "# numbered_handbook_str = handbook.collect_a_handbook(source_ID='OpenWeather')\n",
    "\n",
    "# print(numbered_handbook_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7c7853-e8d3-4984-ac53-3b60917a876d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(HTML(\"\"\"\n",
    "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js\"></script>\n",
    "\"\"\"))\n",
    "\n",
    "sleep(0.1)\n",
    "\n",
    "from itables import init_notebook_mode\n",
    "from itables import show\n",
    "# init_notebook_mode(all_interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bdc053-eaa3-4510-9013-98b29f691d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a545a6b-1456-40b8-a913-b4dfd305c071",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Input task and data desciption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80059925-da23-486a-8111-d3b661811b0b",
   "metadata": {},
   "source": [
    "## Data source 1: OpenStreetMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481b7c40-738c-43bb-a06c-ae6e9daa0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "## task_name ='Nigeria_cities'\n",
    "# downloaded_file_name = r'Nigeria_cities.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the city locations of Nigeria; do not download towns.   \n",
    "# 2. Save the downloaded data as points, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Nigeria_rivers'\n",
    "# downloaded_file_name = r'Nigeria_rivers.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the rivers of Nigeria.   \n",
    "# 2. Save the downloaded data as polylines, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Nigeria_state_boundary'  # most test failed!!!!! Soloved.\n",
    "# downloaded_file_name = r'Nigeria_states_boundary.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all state boundaries of Nigeria.   \n",
    "# # 2. Save the downloaded data as polygons, save it at: {saved_fname} \n",
    "# # '''\n",
    "\n",
    "# task_name ='World_country_boundary'  # most test failed!!!!! Soloved.\n",
    "# downloaded_file_name = r'World_country_boundary.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all country boundaries of the world.   \n",
    "# # 2. Save the downloaded data as polygons, save it at: {saved_fname} \n",
    "# # '''\n",
    "\n",
    "\n",
    "# task_name ='China_mainland_province_boundary'  # most test failed! solved.\n",
    "# downloaded_file_name = r'China_mainland_Province_boundary.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all province boundaries of China mainland.   \n",
    "# 2. Save the downloaded data as polygons in GeoPackage format at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='OSM_PA_boundary'\n",
    "# downloaded_file_name = r'PA_boundary.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the administrative boundary of Pennsylvania State, USA.\n",
    "# 2. Save the downloaded data in GeoPackage format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='OSM_SC_boundary'\n",
    "# downloaded_file_name = r'SC_boundary.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the administrative boundary of South Carolina State, USA.\n",
    "# 2. Save the downloaded data in GeoPackage format at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='OSM_PA_hospital'\n",
    "# downloaded_file_name = r'PA_hospital.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all hospitals in Pennsylvania, USA.\n",
    "# 2. Save the downloaded data as points in GeoPackage format at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='SC_hospital'\n",
    "# downloaded_file_name = r'SC_hospital.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all hospitals in South Carolina, USA.\n",
    "# 2. Save the downloaded data as points in GeoPackage at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='OSM_SC_school'\n",
    "# downloaded_file_name = r'SC_school.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all schools in South Carolina State, USA.\n",
    "# 2. Save the downloaded data as points in GeoPackage format at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='OSM_Yulin_River'\n",
    "# downloaded_file_name = r'Yulin_river.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all rivers in Yulin, Guangxi, China.\n",
    "# 2. Save the downloaded data as polylines in GeoPackage format at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='OSM_CA_park'\n",
    "# downloaded_file_name = r'CA_parks.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all parks in California, USA, including urban public, recreation, state, and national parks.\n",
    "# 2. Save the downloaded data as points in GeoPackage format at: {saved_fname} \n",
    "# # '''\n",
    "\n",
    "## task_name ='OSM_USA_university'\n",
    "# downloaded_file_name = r'USA_universities.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the POIs of all universities, colleges, and other higher education institutions in the USA.\n",
    "# 2. Save the downloaded data as points in GeoPackage format at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='OSM_State_College_street'\n",
    "# downloaded_file_name = r'State_College_street.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all streets of State College, Pennsylvania, USA.\n",
    "# 2. Save the downloaded data as polylines in GeoPackage format at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# ## task_name ='OSM_Nigeria_boundary'\n",
    "# downloaded_file_name = r'Nigeria_boundary.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the administrative boundary of Nigeria.\n",
    "# 2. Save the downloaded data in GeoPackage format at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# ## task_name ='OSM_Afghanistan_boundary'\n",
    "# downloaded_file_name = r'Afghanistan_boundary.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the administrative boundary of Afghanistan.\n",
    "# 2. Save the downloaded data in GeoPackage format at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "\n",
    "# task_name ='OSM_Nigeria_railway'\n",
    "# downloaded_file_name = r'Nigeria_railway.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the railway network of Nigeria.\n",
    "# 2. Save the downloaded data in GeoPackage format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Wuhan_railway_network'\n",
    "# downloaded_file_name = r'Wuhan_Railway_network.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the railway network in Wuhan, Hubei, China.\n",
    "# 2. Save the downloaded data as polylines in GeoPackage format at: {saved_fname} \n",
    "# '''\n",
    "# Wuhan_railway_network is a difficult case! It succeeded at the beginning, but failed all the time later.\n",
    "# The query: area[\"name\"=\"Wuhan\"][\"boundary\"=\"administrative\"]->.searchArea; is not correct. Need to use \"name:en\". \n",
    "# Using \"Hubei Province\" may not return polygons\n",
    "\n",
    "## task_name ='Qingdao_boundary'\n",
    "# downloaded_file_name = r'Qingdao_boundary.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the administrative of Qingdao, Shandong, China.\n",
    "# 2. Save the downloaded data as polygons, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='China_Guangdong_province_boundary'   \n",
    "# downloaded_file_name = r'China_Guangdong_province_boundary.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the Guangdong province boundaries of China.   \n",
    "# 2. Save the downloaded data as polygons, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='OSM_coffee_shop_Vietnam'\n",
    "# downloaded_file_name = r'coffee_shop_Vietnam.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all the coffee shops in Vietnam.\n",
    "# 2. Save the downloaded data in GeoPackage format, save it at: {saved_fname} \n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c84ba-9962-4074-9f32-c355d01d5511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T23:32:40.650197Z",
     "iopub.status.busy": "2024-07-01T23:32:40.650197Z",
     "iopub.status.idle": "2024-07-01T23:32:40.738168Z",
     "shell.execute_reply": "2024-07-01T23:32:40.738168Z",
     "shell.execute_reply.started": "2024-07-01T23:32:40.650197Z"
    }
   },
   "source": [
    "## Data source 2:  US Census Bureau administrative boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df26e3f-82da-4abe-ac6d-3fa7d26d0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_name ='Census_SC_tract'\n",
    "# downloaded_file_name = r'Census_SC_tract.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all census tract boundaries in South Carolina, USA.\n",
    "# 2. Save the downloaded data as polygons in GeoPackage format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Census_SC_blockgroups'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Census_SC_blockgroups.gpkg'\n",
    "# if os.path.exists(saved_fname):\n",
    "#     os.remove(saved_fname)\n",
    "# task = rf'''1. Download all Census block group boundaries in South Carolina, USA.\n",
    "# 2. Save the downloaded data as polygons in GeoPackage format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Census_Centre_boundary'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Census_Centre_boundary.gpkg'\n",
    "# task = rf'''1. Download the administrative boundary of Centre County of Pennsylvania State, USA from Census Bureau.\n",
    "# 2. Save the downloaded data in GeoPackage format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Census_SC_countries_boundary'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Census_SC_counties_boundary.gpkg'\n",
    "# task = rf'''1. Download the administrative boundary of all Counties of South Carolina from Census Bureau.\n",
    "# 2. Save the downloaded data in GeoPackage format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name = \"US_Carolinas_tract\"\n",
    "# downloaded_file_name = r'US_Carolinas_tract.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all the Census tract boundaries of North Carolina and South Carolina in the USA.\n",
    "# 2. Save the downloaded data in GeoPackage format at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='US_county_boundary\"\n",
    "# downloaded_file_name = r'US_county_boundary.gpkg'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download all the county boundaries in the USA.\n",
    "# 2. Save the downloaded data in GeoPackage format at: {saved_fname} \n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc6f56-74dc-4ddd-9951-a8ba631dc18f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T23:37:35.291616Z",
     "iopub.status.busy": "2024-07-01T23:37:35.291616Z",
     "iopub.status.idle": "2024-07-01T23:37:35.379023Z",
     "shell.execute_reply": "2024-07-01T23:37:35.379023Z",
     "shell.execute_reply.started": "2024-07-01T23:37:35.291616Z"
    }
   },
   "source": [
    "## Data source 3:  US Census Bureau demographic variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109efec-9b85-4eeb-9b1b-befe97a38f14",
   "metadata": {},
   "source": [
    "Any user may query small quantities of data with minimal restrictions (up to 50 variables in a single query, and up to 500 queries per IP address per day). However, more than 500 queries per IP address per day requires that you register for an API key. \n",
    "\n",
    "LLM-Find requires an US Census Bureau API key. \n",
    "\n",
    "To request an API key: https://api.census.gov/data/key_signup.html\n",
    "\n",
    "Note: If you request the \"latest\" data in the task, LLMs may have difficult to know what is the \"lastest\" data. If it keeps failing, please use the year (e.g., 2021) explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4bb18b-87af-4110-8a17-0a8b750df499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_name ='Census_SC_counties_population'\n",
    "# downloaded_file_name = r'Census_SC_counties_population.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download 2021 population for each county in South Carolina.\n",
    "# 2. Save the downloaded data as CSV files, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Census_SC_Richland_race_population'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Census_SC_Richland_race_population.csv'\n",
    "# task = rf'''1. Download latest population of each race for Richland county in South Carolina, at Census block group level.\n",
    "# 2. Save the downloaded data as CSV files, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Census_PA_counties_race_population'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Census_PA_counties_race_population.csv'\n",
    "# task = rf'''1. Download latest population by race for all counties in Pennsylvania.\n",
    "# 2. Save the downloaded data as CSV files, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Census_US_states_population'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Census_US_states_population.csv'\n",
    "# task = rf'''1. Download latest population for all states in USA.   \n",
    "# 2. Save the downloaded data as CSV files, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Census_US_states_education_population'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Census_US_states_education_population.csv'\n",
    "# task = rf'''1. Download latest population by higher education attainment over 25 for all states in USA, together with the entire population of each state.   \n",
    "# 2. Save the downloaded data as CSV files, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Census_US_county_household_income'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Census_US_county_household_income.csv'\n",
    "# task = rf'''1. Download the latest median household income data for each county in the USA.   \n",
    "# 2. Save the downloaded data as CSV files, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='Census_US_county_population_by_race'\n",
    "# downloaded_file_name = r'Census_US_county_population_by_race.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the 2022 population by race data for each county in the USA.   \n",
    "# 2. Save the downloaded data as CSV files, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='States_colledge_popultion'   # difficult to get the correct variable combination. Tend to return Male and one age group (e.g., 25-35) only\n",
    "# downloaded_file_name = r'US_state_higher_education_attainment.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the population over 25 years old and the population with a college degree or higher at the state level of USA for 2012 and 2022.\n",
    "# 2. Save the downloaded data in CSV format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='US_SDOH'   # difficult to get the correct variable combination\n",
    "# downloaded_file_name = r'US_SDOH.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the social determinators of health, including: 1) population of each race; 2) Median household income; 3) health insurance coverage; 4) Population of speaking only English at home for the population 5 years and over.\n",
    "# 2. The data should be at the county level in the USA. Year: 2022.\n",
    "# 3. Save the downloaded data in a CSV file, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='US_County_poverty'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\US_County_poverty.csv'\n",
    "# task = rf'''1. Download the ratios of income to all poverty level at the county level in the USA. Year: 2022.\n",
    "# 3. Save the downloaded data in a CSV file, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='Washington_DC_blockgroup_senior_population'\n",
    "# downloaded_file_name = r'Washington_DC_blockgroup_senior_population.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task_name ='Washington_DC_blockgroup_senior_population'\n",
    "# task = rf'''1. Download the senior (older than  65) population groups senior  for all Census blockgroups in Washington D.C., USA.   \n",
    "# 2. Save the downloaded data as CSV files, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='School_enrollment'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\School_enrollment_population.csv'\n",
    "# task = rf'''1. From Census 2020 data, download the school enrollment by level of school for the population 3 years and over of all Census block groups in San Francisco County, California, USA.   \n",
    "# 2. Save the downloaded data as CSV files, save it at: {saved_fname} \n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be421007-ac58-4e43-8a85-6111b842afc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T14:08:49.804272Z",
     "iopub.status.busy": "2024-07-02T14:08:49.804272Z",
     "iopub.status.idle": "2024-07-02T14:08:49.899960Z",
     "shell.execute_reply": "2024-07-02T14:08:49.899960Z",
     "shell.execute_reply.started": "2024-07-02T14:08:49.804272Z"
    }
   },
   "source": [
    "##  Data source 4:  COVID-19 accumulative cases by New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3f323-b738-4401-9dbf-52b3e63a62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_name ='COVID_Richland_SC'\n",
    "# downloaded_file_name = r'COVID_Richland_SC.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the COVID-19 case data of Richland County in South Carolina, USA. The time is from 2021-01 to 2021-09.   \n",
    "# 2. Save the downloaded data as a CSV file at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='COVID_PA'\n",
    "# downloaded_file_name = r'COVID_PA.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the COVID-19 case data of all counties in Pennsylvania, USA. The time is from 2021-10 to 2022-02.   \n",
    "# 2. Save the downloaded data as a CSV file at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='COVID_NY'\n",
    "# downloaded_file_name = r'COVID_NJ_NY.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the COVID-19 case data of all counties in Pennsylvania State and New York State, USA. The period is entire 2021.   \n",
    "# 2. Save the downloaded data as a CSV file at: {saved_fname} \n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827ef639-e2fe-43ae-bb4a-1dfe5a63cf3e",
   "metadata": {},
   "source": [
    "## Data source 5:  Weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c6f0f6-c541-4877-8624-6b6c560ec954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_name ='OpenWeather_Columbia'\n",
    "# downloaded_file_name = r'OpenWeather_Columbia.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the historical weather data of Columbia, South Carolina in August, 2024.\n",
    "# 2. Save the downloaded data in CSV format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# # task_name ='OpenWeather_Yulin_Guangxi'\n",
    "# downloaded_file_name = r'OpenWeather_Yulin_Guangxi.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the historical weather data of Yulin, Guangxi, China, in May 2024.\n",
    "# 2. Save the downloaded data in CSV format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='OpenWeather_Cairo'\n",
    "# downloaded_file_name = r'OpenWeather_Cairo.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the current weather data of Cairo, Egypt.\n",
    "# 2. Save the downloaded data in CSV format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='OpenWeather_Kabul'\n",
    "# downloaded_file_name = r'OpenWeather_Kabul.csv'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the 16-day daily forecast weather data of Kabul, Afghanistan.\n",
    "# 2. Save the downloaded data in CSV format, save it at: {saved_fname} \n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19c9ca-5025-4fb2-88b5-bb54ddd5d57c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T14:09:30.817569Z",
     "iopub.status.busy": "2024-07-02T14:09:30.817569Z",
     "iopub.status.idle": "2024-07-02T14:09:30.912003Z",
     "shell.execute_reply": "2024-07-02T14:09:30.912003Z",
     "shell.execute_reply.started": "2024-07-02T14:09:30.817569Z"
    }
   },
   "source": [
    "## Data source 6:  Satellite image (ESRI World Imagery (for export))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc3209-471b-4e71-b07b-bc60984de039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_name ='FAST_Telescope'\n",
    "# OpenStreetMap Nominatim API cannot return this place now.\n",
    "# It shows on the map, but just cannot be searched! \n",
    "# https://www.openstreetmap.org/query?lat=25.652654&lon=106.856584\n",
    "# Removed by osm-sputnik on Aug. 29, 2024: https://www.openstreetmap.org/way/384699313\n",
    "# downloaded_file_name = r'FAST_Telescope_image.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the FAST Telescope (Guizhou, China) satellite image at level 18.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# # task_name ='Nigeria_image'\n",
    "# downloaded_file_name = r'Nigeria_image.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the Nigeria satellite image at level 7.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# # task_name ='Qingdao_image'\n",
    "# downloaded_file_name = r'Qingdao_image.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the Qingdao, Shandong, China satellite image at level 10.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# ## task_name ='Crescent_Moon_Spring'\n",
    "# downloaded_file_name = r'Crescent_Moon_Spring_image.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the Singing-Sand Mountain and Crescent Moon Spring satellite image at level 16.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='Christ the Redeemer'  # this is a point: a difficult case\n",
    "# downloaded_file_name = r'Christ_the_Redeemer.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the Christ the Redeemer satellite image at level 18.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# #task_name ='Brasília_image'   # not ready yet\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Brasília_image.tif'\n",
    "# task = rf'''1. Download the Brasília satellite image at level 6.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='Japan_image'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Japan_image.tif'\n",
    "# task = rf'''1. Download the Japan satellite image at level 6.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='China'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\China_image.tif'\n",
    "# task = rf'''1. Download the China satellite image at level 6.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name ='YellowStone_National_Park'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Yellow_Stone_National_Park_image.tif'\n",
    "# task = rf'''1. Download the YellowStone National Park satellite image at level 10.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='Hawaii'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Hawaii_image.tif'\n",
    "# task = rf'''1. Download the Hawaii State satellite image at level 7.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='Honolulu'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Honolulu_image.tif'\n",
    "# task = rf'''1. Download the Honolulu satellite image at level 12.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='Kennedy_Space_Center_Visitor_Complex'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Kennedy_Space_Center_Visitor_Complex_image.tif'\n",
    "# task = rf'''1. Download the Kennedy Space Center Visitor Complex satellite image at level 18.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='Hoover_Dam'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Hoover_Dam_image.tif'\n",
    "# task = rf'''1. Download the Hoover Dam satellite image at level 18.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='Nigeria'\n",
    "# downloaded_file_name = 'Nigeria_image.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the Nigeria satellite image at level 7.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='Afghanistan'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Afghanistan_image.tif'\n",
    "# task = rf'''1. Download the Afghanistan satellite image at level 8.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name ='State_college'\n",
    "# saved_fname = r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\State_college_image.tif'\n",
    "# task = rf'''1. Download the State College City, PA satellite image at level 12.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# #task_name = 'Tigard'\n",
    "# downloaded_file_name = 'Tigard_OR_image.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the Tigard, OR satellite image at level 14.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name = \"Xiong'an\"\n",
    "# downloaded_file_name = r'Xiong_an.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the Xiong'an New Area, China satellite image at level 12.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name = \"Penn_State_University\"\n",
    "# downloaded_file_name = r'Penn_State_University.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the Pennsylvania State University satellite image at level 15.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name = \"Mount Everest DOM\"\n",
    "# downloaded_file_name = r'Everest_DOM.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the satellite images of this region [south:27.82, west:86.73, north:28.17, east:87.13] at level 10.\n",
    "# 2. Save the downloaded data in geo-tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# task_name = \"Three Gorges Dam\"\n",
    "downloaded_file_name = r'Three_Gorges_Dam.tif'\n",
    "saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "task = rf'''1. Download the satellite images of Three Gorges Dam in China at level 16.\n",
    "2. Save the downloaded data in geo-tiff format, save it at: {saved_fname} \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31528d-e2db-477e-880c-9efee3d4b2c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T14:29:35.133904Z",
     "iopub.status.busy": "2024-07-17T14:29:35.131903Z",
     "iopub.status.idle": "2024-07-17T14:29:35.149048Z",
     "shell.execute_reply": "2024-07-17T14:29:35.148041Z",
     "shell.execute_reply.started": "2024-07-17T14:29:35.132904Z"
    }
   },
   "source": [
    "## Data source 7:  OpenTopography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c7a38-3f60-4746-bb25-8efaef3fbe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## task_name = \"Wuhan_DEM\"\n",
    "# downloaded_file_name = r'Wuhan_DEM.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the 90m resolution DEM of Wuhan, China from SRTMGL3.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# # task_name = \"Chongqing_DEM\"\n",
    "# downloaded_file_name = r'Chongqing_DEM.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the 30m resolution DEM of Chongqing, China from SRTMGL1.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "# # task_name = \"Hawaii_DEM\"\n",
    "# downloaded_file_name = r'Hawaii_DEM.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the 90m resolution DEM of Hawaii, USA from SRTMGL3.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "\n",
    "## task_name = \"Lhasa, China_DEM\"\n",
    "# downloaded_file_name = r'Lhasa_DEM.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the 30m resolution DEM of Lhasa, China, from COP30.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "\n",
    "# # task_name = \"Iceland\"\n",
    "# downloaded_file_name = r'Iceland_DEM.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the 30m resolution DEM of Iceland_DEM from EU_DTM.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''\n",
    "\n",
    "## task_name = \"Mount Everest\"\n",
    "# downloaded_file_name = r'Everest_DEM.tif'\n",
    "# saved_fname = os.path.join(os.getcwd(), \"Downloaded_Data\", downloaded_file_name)\n",
    "# task = rf'''1. Download the 30m resolution DEM of this region [south:27.82, west:86.73, north:28.17, east:87.13] from AW3D30.\n",
    "# 2. Save the downloaded data in tiff format, save it at: {saved_fname} \n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22ce32-4f33-481c-931e-e0c960c06617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b63f0cd-07d6-4056-9007-c9ee64b312a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T19:44:17.794131Z",
     "iopub.status.busy": "2024-07-02T19:44:17.794131Z",
     "iopub.status.idle": "2024-07-02T19:44:17.885259Z",
     "shell.execute_reply": "2024-07-02T19:44:17.885259Z",
     "shell.execute_reply.started": "2024-07-02T19:44:17.794131Z"
    }
   },
   "source": [
    "# Fetching data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbfdaa9-31ec-4248-a892-9fbe0478f87a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T19:44:05.267712Z",
     "iopub.status.busy": "2024-07-02T19:44:05.267712Z",
     "iopub.status.idle": "2024-07-02T19:44:05.359116Z",
     "shell.execute_reply": "2024-07-02T19:44:05.359116Z",
     "shell.execute_reply.started": "2024-07-02T19:44:05.267712Z"
    }
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490a851-f1f2-4278-ba15-2e63ddd42b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(saved_fname):\n",
    "    os.remove(saved_fname)\n",
    "\n",
    "save_dir = os.path.join(os.getcwd(), \"Downloaded_Data\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model_name = r'gpt-4o'\n",
    "# model_name = r'gpt-4'\n",
    "# model_name = r'gpt-4-turbo'\n",
    "# model_name = r'gpt-3.5-turbo'  # gpt-4-turbo\n",
    "\n",
    "model = ChatOpenAI(api_key=OpenAI_key, model=model_name, temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a5e4b-7a5b-4463-a7ab-d44d34a00c69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T19:44:41.493282Z",
     "iopub.status.busy": "2024-07-02T19:44:41.493282Z",
     "iopub.status.idle": "2024-07-02T19:44:41.582989Z",
     "shell.execute_reply": "2024-07-02T19:44:41.582989Z",
     "shell.execute_reply.started": "2024-07-02T19:44:41.493282Z"
    }
   },
   "source": [
    "## Select the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba05f98-30b2-46d0-9eb2-624f5dbc2754",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "import helper\n",
    "    \n",
    "source_select_prompt_str = helper.create_select_prompt(task=task)\n",
    "\n",
    "print(source_select_prompt_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afccc800-f330-44eb-80e0-676d893111bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-31T22:09:45.195826Z",
     "iopub.status.busy": "2024-07-31T22:09:45.174806Z",
     "iopub.status.idle": "2024-07-31T22:09:45.601124Z",
     "shell.execute_reply": "2024-07-31T22:09:45.577120Z",
     "shell.execute_reply.started": "2024-07-31T22:09:45.195826Z"
    }
   },
   "source": [
    "## Pick up the data source handbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93af39-5b6f-41ce-91fa-700042ac5e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "chunks = []\n",
    "\n",
    "async for chunk in model.astream(source_select_prompt_str):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "# clear_output(wait=True)\n",
    "clear_output(wait=False)\n",
    "LLM_reply_str = helper.convert_chunks_to_str(chunks=chunks)\n",
    "\n",
    "print(\"Select the data source: \\n\")\n",
    "print(LLM_reply_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e9b9cc-6f87-4783-9b45-f04e8e66bd6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T19:45:05.791108Z",
     "iopub.status.busy": "2024-07-02T19:45:05.791108Z",
     "iopub.status.idle": "2024-07-02T19:45:05.885257Z",
     "shell.execute_reply": "2024-07-02T19:45:05.885257Z",
     "shell.execute_reply.started": "2024-07-02T19:45:05.791108Z"
    }
   },
   "source": [
    "## Generate the data fetching program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5012ca-954c-406a-8e46-7a2eb2f252c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "select_source = ast.literal_eval(LLM_reply_str)\n",
    "\n",
    "selected_data_source= select_source['Selected data source']\n",
    "\n",
    "handbook_files = handbook.collect_handbook_files()\n",
    "descriptions_str, data_source_dict = handbook.assemble_handbook_description(handbook_files)\n",
    "\n",
    "data_source_ID = data_source_dict[selected_data_source]['ID']\n",
    "\n",
    "print(\"selected_data_source:\", selected_data_source)\n",
    "print(\"data_source_ID:\", data_source_ID)\n",
    "\n",
    "# handbook_list = constants.handbooks[f\"{data_source_ID}\"]\n",
    "# handbook_str =  '\\n'.join([f\"{idx + 1}. {line}\" for idx, line in enumerate(handbook_list)])\n",
    "handbook_str = handbook.collect_a_handbook(source_ID=data_source_ID)\n",
    "\n",
    "print()\n",
    "print(f\"Handbook:\\n{handbook_str}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83929d-ae9f-4c3c-aaf0-ee0edbdb93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "download_prompt_str = helper.create_download_prompt(task, selected_data_source, handbook_str)\n",
    "\n",
    "print(download_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dd534a-3dd6-4292-a12e-4417284ad9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "chunks = []\n",
    "\n",
    "async for chunk in model.astream(download_prompt_str):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "# clear_output(wait=True)\n",
    "clear_output(wait=False)\n",
    "LLM_reply_str = helper.convert_chunks_to_str(chunks=chunks)\n",
    "print(LLM_reply_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc63c3-9eb3-40ad-8daa-fefa5ba85190",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = helper.extract_code_from_str(LLM_reply_str, task)\n",
    "display(Code(code, language='python'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288069a-6deb-4bbb-bbf9-1998cf823b9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T19:45:44.349836Z",
     "iopub.status.busy": "2024-07-02T19:45:44.349836Z",
     "iopub.status.idle": "2024-07-02T19:45:44.441835Z",
     "shell.execute_reply": "2024-07-02T19:45:44.441835Z",
     "shell.execute_reply.started": "2024-07-02T19:45:44.349836Z"
    }
   },
   "source": [
    "# Execute the generated program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ff924-8af3-4b1f-aed6-859033500f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %autoreload 2\n",
    "# %load_ext autoreload\n",
    "\n",
    "code = code.replace('area({osm_id})->.searchArea;', 'relation({osm_id}); map_to_area->.searchArea;')  # GPT-4o never follow the related instruction!\n",
    "    \n",
    "code = helper.execute_complete_program(code=code, try_cnt=10, task=task, model_name=model_name, handbook_str=handbook_str)\n",
    "\n",
    "code = code.replace('area({osm_id})->.searchArea;', 'relation({osm_id}); map_to_area->.searchArea;')  # GPT-4o never follow the related instruction!\n",
    "\n",
    "\n",
    "display(Code(code, language='python'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f92ee-4998-4b1a-a55e-7f2267a49b4a",
   "metadata": {},
   "source": [
    "# Show the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119c498-86f1-4041-b3d1-ae9d85ea9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_name = saved_fname[-4:]\n",
    "print(\"Saved file:\", saved_fname)\n",
    "print(\"ext_name:\", ext_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2405ec33-5d01-498e-8735-6bd1fa35a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if ext_name == '.csv':\n",
    "    df = pd.read_csv(saved_fname, encoding='ISO-8859-1')\n",
    "    results = df\n",
    "\n",
    "if ext_name == 'gpkg':\n",
    "    gdf = gpd.read_file(saved_fname)\n",
    "    results = gdf\n",
    "    # gdf.plot()\n",
    "    # gdf.explore()\n",
    "\n",
    "if selected_data_source == 'ESRI World Imagery (for Export)':\n",
    "    results = Image.open(saved_fname)\n",
    "\n",
    "if selected_data_source == 'OpenTopography':\n",
    "    results = Image.open(saved_fname)\n",
    "    plt\n",
    "\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebfd8fe-0766-4f90-9c61-99ac47603dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import leafmap.leafmap as leafmap\n",
    "import leafmap.foliumap as leafmap\n",
    "\n",
    "# m = leafmap.Map(google_map=\"HYBRID\")\n",
    "# m = leafmap.Map()\n",
    "\n",
    "if ext_name == 'gpkg':\n",
    "    gdf = gpd.read_file(saved_fname)  \n",
    "    bounding_box = gdf.total_bounds\n",
    "    m = leafmap.Map()     \n",
    "    m.add_gdf(gdf, layer_name=\"Downloaded_data\")\n",
    "    m.fit_bounds([[bounding_box[1], bounding_box[0]], [bounding_box[3], bounding_box[2]]])\n",
    "\n",
    "if selected_data_source == 'OpenTopography':\n",
    "    # m = leafmap.Map()\n",
    "    m = leafmap.Map(google_map=\"HYBRID\")\n",
    "    m.add_raster(source=saved_fname, layer_name='OpenTopography DEM', colormap='viridis') \n",
    "\n",
    "if selected_data_source == 'ESRI World Imagery (for Export)':\n",
    "\n",
    "    '''\n",
    "    Note: The downloaded images use the map projection as the data source - Web Mercator. If you find that the downloaded image does not align the basemap correctly, you mind need to use ArcGIS or QGIS to show the image. It seems that some interative map widgets cannot recognize and overlay the Web Mercator in some occasions.\n",
    "    '''\n",
    "   \n",
    "    m = leafmap.Map()\n",
    "    # m.add_basemap(\"HYBRID\")\n",
    "    m.add_basemap(\"Esri.WorldImagery\")\n",
    "    m.add_raster(source=saved_fname, layer_name='ESRI World Imagery') \n",
    "   # m.add_raster(source=saved_fname, layer_name='Esri.WorldImagery')\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea061e-988e-470e-b648-ed831be180b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f3e95-adb7-4c66-97f2-e4f656a9e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you do not like to install Leafmap or have difficulty to install it, \n",
    "### just use gdf.explore() to check the downloaded data in an interative way\n",
    "# gdf.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7982bf4-4e61-47d5-856a-f981a29a4226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a948b-8dd4-4a35-9510-e84667e6b935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_geo",
   "language": "python",
   "name": "llm_geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
