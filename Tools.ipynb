{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac890b9c-cd22-43e1-9772-f050a4105018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T01:27:39.340046Z",
     "iopub.status.busy": "2024-08-03T01:27:39.339045Z",
     "iopub.status.idle": "2024-08-03T01:27:43.722620Z",
     "shell.execute_reply": "2024-08-03T01:27:43.722620Z",
     "shell.execute_reply.started": "2024-08-03T01:27:39.340046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Bounding Box in Web Mercator: (12523442.714243276, 0.0, 17532819.799940586, 7514065.628545966)\n",
      "E:/OneDrive_PSU/OneDrive - The Pennsylvania State University/Research_doc/LLM-Find/Downloaded_Data/Japan_image.tif\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import osmnx as ox\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from osgeo import gdal\n",
    "import os\n",
    "from pyproj import Transformer\n",
    "import rasterio\n",
    "\n",
    "\n",
    "def download_data():\n",
    "    # Define the target area for which to download the data\n",
    "    place = 'Japan'\n",
    "\n",
    "    # Get the boundary of Japan\n",
    "    # get the first return's bounding box\n",
    "    url = f\"https://nominatim.openstreetmap.org/search?q={place}&format=geojson\"\n",
    "    response = requests.get(url, headers={\"User-Agent\":\"LLM-Find/gladcolor@gmail.com\"})\n",
    "    minx, miny, maxx, maxy = response.json()['features'][0]['bbox']\n",
    "\n",
    "    # extend the boundary for a point or to small:\n",
    "    if abs(maxx - minx) < 0.000001:  # note unit is degree\n",
    "         ext = 0.00005\n",
    "         maxx = maxx + ext\n",
    "         minx = minx - ext\n",
    "         maxy = maxy + ext\n",
    "         miny = miny - ext \n",
    "        \n",
    "    # Set the zoom level\n",
    "    z = 4\n",
    "    n = 2 ** z\n",
    "\n",
    "    # Calculate the tiling scheme boundaries\n",
    "    tile_min_col = int((minx + 180.0) / 360.0 * n)\n",
    "    tile_max_col = int((maxx + 180.0) / 360.0 * n)\n",
    "    tile_min_row = int((1.0 - np.log(np.tan(np.radians(maxy)) + 1 / np.cos(np.radians(maxy))) / np.pi) / 2.0 * n)\n",
    "    tile_max_row = int((1.0 - np.log(np.tan(np.radians(miny)) + 1 / np.cos(np.radians(miny))) / np.pi) / 2.0 * n)\n",
    "\n",
    "    # Create directory to store individual tile images\n",
    "    save_dir = \"tiles\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Download tiles\n",
    "    for row in range(tile_min_row, tile_max_row + 1):\n",
    "        for col in range(tile_min_col, tile_max_col + 1):\n",
    "            url = f\"https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{row}/{col}\"\n",
    "            response = requests.get(url)\n",
    "            tile_path = os.path.join(save_dir, f\"{z}-{row}-{col}.jpg\")\n",
    "            with open(tile_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "    # Stitch the tiles to create a mosaic image\n",
    "    cols = tile_max_col - tile_min_col + 1\n",
    "    rows = tile_max_row - tile_min_row + 1\n",
    "    tile_width, tile_height = Image.open(os.path.join(save_dir, f\"{z}-{tile_min_row}-{tile_min_col}.jpg\")).size\n",
    "    mosaic = Image.new('RGB', (cols * tile_width, rows * tile_height))\n",
    "\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            tile_path = os.path.join(save_dir, f\"{z}-{tile_min_row + row}-{tile_min_col + col}.jpg\")\n",
    "            tile = Image.open(tile_path)\n",
    "            mosaic.paste(tile, (col * tile_width, row * tile_height))\n",
    "\n",
    "    # Save the mosaic image as a TIFF file\n",
    "    image_array = np.array(mosaic)\n",
    "\n",
    "        # Calculate the actual geographic bounds of the mosaic\n",
    "    def tile_to_lonlat(col, row, zoom):\n",
    "        n = 2.0 ** zoom\n",
    "        lon_deg = col / n * 360.0 - 180.0\n",
    "        lat_rad = np.arctan(np.sinh(np.pi * (1 - 2 * row / n)))\n",
    "        lat_deg = np.degrees(lat_rad)\n",
    "        return lon_deg, lat_deg\n",
    "\n",
    "    actual_min_lon, actual_max_lat = tile_to_lonlat(tile_min_col, tile_min_row, z)\n",
    "    actual_max_lon, actual_min_lat = tile_to_lonlat(tile_max_col + 1, tile_max_row + 1, z)\n",
    "\n",
    "       # Convert geographic bounds to Web Mercator\n",
    "    transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:3857\", always_xy=True)\n",
    "    actual_min_x, actual_min_y = transformer.transform(actual_min_lon, actual_min_lat)\n",
    "    actual_max_x, actual_max_y = transformer.transform(actual_max_lon, actual_max_lat)\n",
    "\n",
    "    # Set the bounding box for Web Mercator\n",
    "    actual_bounding_box_mercator = actual_min_x, actual_min_y, actual_max_x, actual_max_y\n",
    "    print(\"Actual Bounding Box in Web Mercator:\", actual_bounding_box_mercator)\n",
    "    \n",
    "    transform = from_bounds(*actual_bounding_box_mercator, width=mosaic.width, height=mosaic.height)\n",
    "\n",
    "    mosaic_path = \"E:/OneDrive_PSU/OneDrive - The Pennsylvania State University/Research_doc/LLM-Find/Downloaded_Data/Japan_image.tif\"\n",
    "    crs = {'init': 'epsg:3857'}\n",
    "\n",
    "    # Save the image as a GeoTIFF using rasterio\n",
    "    print(mosaic_path)\n",
    "    with rasterio.open(\n",
    "        mosaic_path,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=image_array.shape[0],\n",
    "        width=image_array.shape[1],\n",
    "        count=image_array.shape[2],  # Number of channels (e.g., 3 for RGB)\n",
    "        dtype=image_array.dtype,\n",
    "        crs=crs,\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        # Write each channel separately\n",
    "        for i in range(1, image_array.shape[2] + 1):\n",
    "            print(i)\n",
    "            dst.write(image_array[:, :, i - 1], i)\n",
    " \n",
    "    # Clean up the individual tile images (optional)\n",
    "    for file in os.listdir(save_dir):\n",
    "        os.remove(os.path.join(save_dir, file))\n",
    "    os.rmdir(save_dir)\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba34b1-68e1-4ad4-a267-dd4a5fd593eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def download_data():\n",
    "    # Define the endpoint and related configurations\n",
    "    base_url = \"https://api.census.gov/data/2021/acs/acs5\"\n",
    "    api_key = \"{US_Census_key}\"\n",
    "    dataset_year = \"2021\"\n",
    "    dataset_source = f\"ACS {dataset_year}\"\n",
    "\n",
    "    # Variables we need to fetch\n",
    "    variables = [\n",
    "        # B02001_001E: Total population\n",
    "        \"B02001_001E\",\n",
    "        # B02001_002E: White alone\n",
    "        \"B02001_002E\",\n",
    "        # B02001_003E: Black or African American alone\n",
    "        \"B02001_003E\",\n",
    "        # B02001_004E: American Indian and Alaska Native alone\n",
    "        \"B02001_004E\",\n",
    "        # B02001_005E: Asian alone\n",
    "        \"B02001_005E\",\n",
    "        # B02001_006E: Native Hawaiian and Other Pacific Islander alone\n",
    "        \"B02001_006E\",\n",
    "        # B02001_007E: Some other race alone\n",
    "        \"B02001_007E\",\n",
    "        # B02001_008E: Two or more races\n",
    "        \"B02001_008E\",\n",
    "    ]\n",
    "\n",
    "    # Fetch the variable labels\n",
    "    response = requests.get(f\"{base_url}/variables.json\")\n",
    "    print(f\"{base_url}/variables.json\")\n",
    "    \n",
    "    response.raise_for_status()\n",
    "    # print(response.text[:10000])\n",
    "    variables_metadata = response.json()\n",
    "\n",
    "    # Helper function to get variable labels\n",
    "    def get_variable_label(var_name):\n",
    "        label = variables_metadata['variables'][var_name]['label']\n",
    "        return label.replace(\"Estimate!!\", \"\").strip()\n",
    "\n",
    "    # Construct the URL for the data request\n",
    "    get_vars = \",\".join(variables)\n",
    "    url = f\"{base_url}?get={get_vars}&for=county:*&in=state:*&key={api_key}\"\n",
    "\n",
    "    # Download data from Census API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    # Prepare the CSV file for writing\n",
    "    csv_path = \"E:\\\\OneDrive_PSU\\\\OneDrive - The Pennsylvania State University\\\\Research_doc\\\\LLM-Find\\\\Python_code\\\\Downloaded_Data\\\\Census_US_county_population_by_race.csv\"\n",
    "\n",
    "    # Create header with variable labels\n",
    "    header = [f\"{var}:{get_variable_label(var)}\" for var in variables] + [\"state_fips\", \"county_fips\", \"year\", \"source\"]\n",
    "    rows = data[1:]  # Skip the header row provided by API\n",
    "    for row in rows:\n",
    "        row.extend([dataset_year, dataset_source])\n",
    "\n",
    "    # Write to CSV file\n",
    "    with open(csv_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "# Execute the function\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2ae7cd8-ea6f-4354-a5fc-5a9a57639c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T17:31:40.472265Z",
     "iopub.status.busy": "2024-09-06T17:31:40.471284Z",
     "iopub.status.idle": "2024-09-06T17:31:41.730430Z",
     "shell.execute_reply": "2024-09-06T17:31:41.730430Z",
     "shell.execute_reply.started": "2024-09-06T17:31:40.472265Z"
    }
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 2 column 1 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\ox\\Lib\\site-packages\\requests\\models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\ox\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\ox\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\ox\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 1)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwriterows(rows)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Execute the function\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[43mdownload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[46], line 48\u001b[0m, in \u001b[0;36mdownload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m     47\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m---> 48\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Prepare the CSV file for writing\u001b[39;00m\n\u001b[0;32m     51\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCensus_SC_Richland_race_population.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32me:\\ProgramData\\Anaconda3\\envs\\ox\\Lib\\site-packages\\requests\\models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 1)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "\n",
    "def download_data():\n",
    "    # Define the endpoint and related configurations\n",
    "    base_url = \"https://api.census.gov/data/2022/acs/acs5\"\n",
    "    api_key = \"xxxx\"\n",
    "    dataset_year = \"2021\"\n",
    "    dataset_source = f\"ACS {dataset_year}\"\n",
    "\n",
    "    # Variables we need to fetch\n",
    "    variables = [\n",
    "        # B02001_001E:Total population\n",
    "        \"B02001_001E\",\n",
    "        # B02001_002E:White alone\n",
    "        \"B02001_002E\",\n",
    "        # B02001_003E:Black or African American alone\n",
    "        \"B02001_003E\",\n",
    "        # B02001_004E:American Indian and Alaska Native alone\n",
    "        \"B02001_004E\",\n",
    "        # B02001_005E:Asian alone\n",
    "        \"B02001_005E\",\n",
    "        # B02001_006E:Native Hawaiian and Other Pacific Islander alone\n",
    "        \"B02001_006E\",\n",
    "        # B02001_007E:Some other race alone\n",
    "        \"B02001_007E\",\n",
    "        # B02001_008E:Two or more races\n",
    "        \"B02001_008E\",\n",
    "    ]\n",
    "\n",
    "    # Fetch the variable labels\n",
    "    response = requests.get(f\"{base_url}/variables.json\")\n",
    "    variables_metadata = response.json()\n",
    "\n",
    "    # Helper function to get variable labels\n",
    "    def get_variable_label(var_name):\n",
    "        label = variables_metadata['variables'][var_name]['label']\n",
    "        return label.replace(\"Estimate!!\", \"\").strip()\n",
    "\n",
    "    # Construct the URL for the data request\n",
    "    get_vars = \",\".join(variables)\n",
    "    url = f\"{base_url}?get={get_vars}&for=block%20group:*&in=state:45 county:079&key={api_key}\"\n",
    "\n",
    "    # Download data from Census API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    # Prepare the CSV file for writing\n",
    "    csv_path = \"Census_SC_Richland_race_population.csv\"\n",
    "\n",
    "    # Create header with variable labels\n",
    "    header = [f\"{var}:{get_variable_label(var)}\" for var in variables] + [\"state_fips\", \"county_fips\", \"tract_fips\", \"block_group_fips\", \"year\", \"source\"]\n",
    "    rows = data[1:]  # Skip the header row provided by API\n",
    "    for row in rows:\n",
    "        row.extend([dataset_year, dataset_source])\n",
    "\n",
    "    # Write to CSV file\n",
    "    with open(csv_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "# Execute the function\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ba153a9-2331-4dc3-b807-a3c7ded110d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T21:07:12.305584Z",
     "iopub.status.busy": "2024-08-02T21:07:12.305584Z",
     "iopub.status.idle": "2024-08-02T21:07:12.321538Z",
     "shell.execute_reply": "2024-08-02T21:07:12.321538Z",
     "shell.execute_reply.started": "2024-08-02T21:07:12.305584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.transform import from_bounds\n",
    "\n",
    "# Load the image using PIL\n",
    "pil_image = Image.open(r\"E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Python_code\\Downloaded_Data\\Everest_DOM.tif\")\n",
    "\n",
    "# Convert the PIL image to a NumPy array\n",
    "image_array = np.array(pil_image)\n",
    "\n",
    "# Define the bounding box (left, bottom, right, top)\n",
    "# Example coordinates (in desired CRS units, e.g., meters or degrees)\n",
    "bounding_box = 86.73, 27.82, 87.13, 28.17# from_bounds(west, south, east, north, width, height)\n",
    "\n",
    "# Define the transform for the GeoTIFF based on the bounding box\n",
    "transform = from_bounds(*bounding_box, width=pil_image.width, height=pil_image.height)\n",
    "\n",
    "# Define the CRS (coordinate reference system) for the output GeoTIFF\n",
    "# Example using WGS 84 (EPSG:4326), but change as needed for your data\n",
    "crs = {'init': 'epsg:4326'}\n",
    "\n",
    "# Save the image as a GeoTIFF using rasterio\n",
    "with rasterio.open(\n",
    "    r'E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Python_code\\Downloaded_Data\\output_image.tif',\n",
    "    'w',\n",
    "    driver='GTiff',\n",
    "    height=image_array.shape[0],\n",
    "    width=image_array.shape[1],\n",
    "    count=image_array.shape[2],  # Number of channels (e.g., 3 for RGB)\n",
    "    dtype=image_array.dtype,\n",
    "    crs=crs,\n",
    "    transform=transform\n",
    ") as dst:\n",
    "    # Write each channel separately\n",
    "    for i in range(1, image_array.shape[2] + 1):\n",
    "        print(i)\n",
    "        dst.write(image_array[:, :, i - 1], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e3e507f-52bc-4103-9662-68eeab77fc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The following code is to download the railway network in Wuhan, Hubei, China.\n",
      "# Import necessary libraries\n",
      "import geopandas as gpd\n",
      "import requests\n",
      "import json\n",
      "import osmnx as ox\n",
      "\n",
      "def download_data():\n",
      "    # Define the area for Wuhan, Hubei, China\n",
      "    place_name = \"Wuhan, Hubei, China\"\n",
      "\n",
      "    # Get the bounding box of Wuhan using OSMnx\n",
      "    gdf = ox.geocode_to_gdf(place_name)\n",
      "    west, south, east, north = gdf.unary_union.bounds\n",
      "\n",
      "    # Define Overpass API query to get railway network in the bounding box\n",
      "    overpass_url = \"https://overpass-api.de/api/interpreter\"\n",
      "    overpass_query = f \"\"\"    [out:json];\n",
      "    (\n",
      "        way[\"railway\"]({south},{west},{north},{east});\n",
      "        relation[\"railway\"]({south},{west},{north},{east});\n",
      "    );\n",
      "    out geom;\n",
      "      \"\"\"    # Send request to Overpass API\n",
      "    response = requests.get(overpass_url, params={'data': overpass_query})\n",
      "    response.raise_for_status()  # Automatically raises an error for bad status codes\n",
      "\n",
      "    # Parse the JSON response\n",
      "    data = response.json()\n",
      "\n",
      "    # Extract elements with their geometries\n",
      "    features = []\n",
      "    for element in data['elements']:\n",
      "        if 'geometry' in element:\n",
      "            points = [(point['lon'], point['lat']) for point in element['geometry']]\n",
      "            # Convert all property values to str\n",
      "            properties = {\n",
      "                key: ', '.join(map(str, value)) if isinstance(value, list) else str(value)\n",
      "                for key, value in element.items() if key != 'geometry'\n",
      "            }\n",
      "\n",
      "            if element['type'] == 'way':\n",
      "                features.append({\n",
      "                    'type': 'Feature',\n",
      "                    'geometry': {'type': 'LineString', 'coordinates': points},\n",
      "                    'properties': properties\n",
      "                })\n",
      "            elif element['type'] == 'relation':\n",
      "                for member in element['members']:\n",
      "                    if member['type'] == 'way' and 'geometry' in member:\n",
      "                        points = [(point['lon'], point['lat']) for point in member['geometry']]\n",
      "                        properties = {\n",
      "                            key: ', '.join(map(str, value)) if isinstance(value, list) else str(value)\n",
      "                            for key, value in element.items() if key != 'geometry'\n",
      "                        }\n",
      "                        features.append({\n",
      "                            'type': 'Feature',\n",
      "                            'geometry': {'type': 'LineString', 'coordinates': points},\n",
      "                            'properties': properties\n",
      "                        })\n",
      "\n",
      "    # Create a GeoDataFrame\n",
      "    gdf_railway = gpd.GeoDataFrame.from_features(features, crs='EPSG:4326')\n",
      "\n",
      "    # Save to GeoPackage\n",
      "    output_file = r\"E:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Downloaded_Data\\Wuhan_Railway_network.gpkg\"\n",
      "    gdf_railway.to_file(output_file, layer='railway_network', driver='GPKG')\n",
      "\n",
      "# Execute the function\n",
      "download_data()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import toml\n",
    "\n",
    "handbook_file = r'D:\\OneDrive_PSU\\OneDrive - The Pennsylvania State University\\Research_doc\\LLM-Find\\Python_code\\Handbooks\\OpenStreetMap.toml'\n",
    "\n",
    "handbook = toml.load(handbook_file)\n",
    "print(handbook['code_example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7444f6f-04dd-487c-9a27-55f05a6f2e36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T16:52:47.136528Z",
     "iopub.status.busy": "2024-09-06T16:52:47.136528Z",
     "iopub.status.idle": "2024-09-06T16:52:47.140921Z",
     "shell.execute_reply": "2024-09-06T16:52:47.140921Z",
     "shell.execute_reply.started": "2024-09-06T16:52:47.136528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keys\\\\config.ini'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install toml\n",
    "os.path.join('Keys', 'config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1deb198a-42a4-4314-8efe-c5a1d67a4c62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T17:52:43.443382Z",
     "iopub.status.busy": "2024-09-06T17:52:43.443382Z",
     "iopub.status.idle": "2024-09-06T17:52:43.448090Z",
     "shell.execute_reply": "2024-09-06T17:52:43.448090Z",
     "shell.execute_reply.started": "2024-09-06T17:52:43.443382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView(<Section: API_Key>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "import LLM_Find_Codebase as codebase\n",
    "import os\n",
    "config = configparser.ConfigParser()\n",
    "config.read(os.path.join('Keys', 'US_Census_demography.keys'))\n",
    "# config.read('Keys/OpenWeather.keys')\n",
    "# config.read('Keys/config.ini')\n",
    "config['API_Key'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c317afde-f5d8-4fc8-ba9a-75b132d74f5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T17:52:45.130393Z",
     "iopub.status.busy": "2024-09-06T17:52:45.130393Z",
     "iopub.status.idle": "2024-09-06T17:52:45.134669Z",
     "shell.execute_reply": "2024-09-06T17:52:45.134669Z",
     "shell.execute_reply.started": "2024-09-06T17:52:45.130393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView(<Section: API_Key>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['API_Key'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca1a7a00-2ae2-426a-8c43-b11487e7fc6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-06T18:06:54.608033Z",
     "iopub.status.busy": "2024-09-06T18:06:54.608033Z",
     "iopub.status.idle": "2024-09-06T18:06:54.611882Z",
     "shell.execute_reply": "2024-09-06T18:06:54.611882Z",
     "shell.execute_reply.started": "2024-09-06T18:06:54.608033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us_census_demography\n"
     ]
    }
   ],
   "source": [
    "for key in config['API_Key'].keys():\n",
    "    value = print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8660074f-c4c7-4eb3-ba17-bdfe147fa1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_dict = handbook.load_keys(source_ID='US_Census_demography', keys_dir='Keys')\n",
    "print(keys_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "ox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
